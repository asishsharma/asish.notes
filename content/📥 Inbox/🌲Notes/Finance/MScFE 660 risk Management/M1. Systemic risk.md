

At the end of this module, students will be able to:

-   Critique (systemic) risk measurement techniques that assume stationarity.
-   Analyze a system and test for systemic risk with Python using a variety of measures, e.g., absorption ratio, eigenvector centrality, distressed insurance premium, marginal and systemic expected shortfall, and others.
-   Detect and appraise a government’s implicit guarantee of a financial program or security (e.g., the “too big to fail” problem) and plan accordingly.
-   Predict the impact of financial shocks to a financial system composed of dense interrelationships


# 1.1 **SYSTEMIC RISK: MACROECONOMIC INDICATORS**
## Required Readings

Required Readings for this program are open access, which means you should be able to access them at no cost. The link provided in the citation will take you directly to the reading or to a page where you can download it.

1.  Bisias, Dimitrios, et al. “A Survey of Systemic Risk Analytics.” _Annual Review of Financial Economics_, vol. 4, no. 1, Oct 2012, pp. 255–296, [https://dspace.mit.edu/bitstream/handle/1721.1/87772/Lo_A%20survey.pdf?sequence=1&isAllowed=y](https://dspace.mit.edu/bitstream/handle/1721.1/87772/Lo_A%20survey.pdf?sequence=1&isAllowed=y).

-   **Required pages:** pp. 1–56
-   **Estimated reading time:** 1 hour 15 minutes
**Reading Time**

30 minutes

**Prior Knowledge**

The Great Financial Crisis

**Keywords**

Non-stationarity, Shadow banking, Disintermediation, Systemic risk, Macroprudential measures, Microprudential measures, Emergent property, Complexity, Network, Contagion

_In Portfolio Theory, we discussed portfolio construction as a balancing act between profit maximization and drawdown or loss minimization. The portfolio theories consisted of relatively straightforward models of return dynamics and were mostly focused on equities. Moreover, the approaches were largely premised on the persistence of basic historical statistical phenomena. In this course, we expand our definition of risk several times over. We accomplish this not only by taking into account additional asset classes such as credit and traded volatility products but we also step back to survey the financial ecosystem as a whole (that is, as a network) and appreciate the types of risk that may threaten this delicate system. More to the point, we take up the various methods for quantifying these risks and managing them._

## **1. Financial Engineering as Risk Management**

Engineering might be considered the science of problem solving. More specifically, it is the science of problem solving with technical and quantitative tools, which include various models and algorithms. In fact, as we have seen over the course of the program, we find ourselves solving disparate financial problems by applying a growing set of tools, which themselves are evolving. The problem of risk is in many ways the perfect challenge for the financial engineer. Does not the following description of systemic risk management sound like an accurate description of financial engineering’s more general endgame?

“A practical implementation” that translates “economic concepts into very particular choices: one must decide which attributes of which entities will be measured, how frequently and over what observation interval, and with what levels of granularity and accuracy. Summary measures involve further choices on how to filter, transform, and aggregate the raw inputs” (Bisias 2).

In this module and in the rest of this course, we seek to apply the techniques we have already learned, but in new ways. In doing so, we come to appreciate how generalizable these techniques are. Of course, we will also sharpen our expertise with those tools that are recognized to serve us especially well in the risk management discipline, but in one sense, risk management is just another domain to practice the type of technical thinking that we have been learning throughout this program. This is not to say that risk management is not, in so many ways, a uniquely vital and formidable challenge. It is only to say that the engineering of risk management is not essentially different from the engineering we have been learning to date. Conversely, the critical thinking and quantitative tools applied to the assessment of a wide range risks should be kept in mind for potential application in other areas of finance. Throughout the rest of this course, try to stay alert to the accomplishment of both of these objectives.

## **2. The Curse of Non-Stationarity**

Risk managers are called to play several roles. Certainly, they must make “backward-looking” predictions, such as Value at Risk based on historical data, whether this is a non-parametric simulation based on past data points, a parametric simulation based on a past distribution, or an expected distribution based on past observations. There are many quantitative tools to employ for these tasks, and we have examined many of these in the last and previous courses—and we will again in this course as well.

We can learn a lot from history, and from historical data specifically. However, assuming with too much confidence that the future will resemble the past can be dangerous. For this reason, the historically focused, backward-looking type of risk management is often disparaged as "driving by looking in the rear-view mirror." So a good risk manager, like a good portfolio manager, must also be looking _forward: _at the emerging possibilities and probabilities that do not have a direct precedent in the historical data. We focus much more on this type of risk management in this course. The subject of the current lesson, the measurement and management of systemic risk, belongs mostly to this forward-looking category of responsibilities. Underlining this point, Bisias et al. go so far as to say, "In fact, the very notion of systemic risk is a good illustration of non-stationarity" (29).

## **3. Diversity, Complexity and Networks**

As Bisias et al. point out, systemic risk may take any of several forms, and it has many definitions, and due to the infrequency of systemic shocks, we have a precious little data to support our investigations (2, 4). As we witnessed in the application of evolutionary algorithms in previous courses, these competing models and definitions of systemic risk can actually enrich our overall understanding. Indeed, Bisias et al. announce the need for “a robust framework for monitoring and managing financial stability,” which “must incorporate both a diversity of perspectives and a continuous process for re-evaluating the evolving structure of the financial system and adapting systemic risk measures to these changes” (2). In this sense, diversity of thought and evolutionary progress go hand in hand.

One aspect of the challenge of defining, recognizing, or measuring systemic risk is that it is often the result of the accumulation of particularities that by themselves would not pose a risk. On this point, Bisias says that "the fallacy of composition applies: patterns exist in market dynamics at the system level that are distinct from the simple aggregation of the behavior of the individual participants" (12). In other words, systemic risk is an "emergent property," a feature of a complex system that only arises out of the dense web of interaction between the elements of the system.

For an illustration of such a web, look no further than figure 1. In the next lesson, we will learn more about how "legal entities and the financial contracts between them are basic building blocks for understanding and modeling systemic risk" (Bisias 57) and how Network Graphs help us visualize such a model. Note that the upper and lower network graphs resemble each other, but look more closely to observe how many more connections appear in the lower network graph, indicating that the Chinese interbank system is becoming increasingly dense with each passing year. A network graph of interbank connections at the global or some other regional level would not look much different and would certainly exhibit the same worrying trend toward densification. Such visualizations are especially helpful when discussing systemic risk because they highlight the potential for contagion in the financial system: Without having direct counterparty exposure to any particular market participant, most of the banks shown still have a level of indirect exposure to most of the other banks. In the aggregate, this secondary or tertiary indirect exposure could be massive. You should expect to see in this course much more discussion of graph and network theory as tools for understanding the transmission of risk. We will use these tools not just for visualizing the possibilities but for estimating the probabilities of events based on conditioning events which are modeled as nodes on a similar graph.

**Figure 1: Chinese Interbank Connections Network**

![](https://assets.gathercontent.com/NTQ2NTg/GOvNglBErD8XGNen?auto=format%2Ccompress&fit=max&h=800&q=75&s=c696533207807e5bdfde67a80fe0e8ef)

Source: Qi, Ming, et al. "Interconnectedness and Systemic Risk Measures of Chinese Financial Institutions." _Kybernetes_, 2022, vol. 51. no. 13, pp. 57–81, [https://www.emerald.com/insight/content/doi/10.1108/K-04-2021-0270/full/html](https://www.emerald.com/insight/content/doi/10.1108/K-04-2021-0270/full/html).

## **4. Shadow Banking**

Note too in the above graphic that only inter_bank_ connections are shown; we do not see the asset managers, hedge funds, or other investment firms. There would be much too many such entities to fit in a single graphic. Adding the connections between all of these market participants would mean showing such a dense layer that there would hardly be any white space left in the middle of the graphic.

These other financial entities more or less form what is referred to as the **shadow banking** sector in the reading. It must be said that this is a somewhat loaded description: Activity that occurs "in the shadows" usually implies that the activity is unethical, illegal or otherwise inappropriate. There are many definitions of shadow banking (Schwarz 620–1), some of which may in fact include this type of characterization. However in its strictest sense, it simply refers to when non-bank entities conduct the financing that is traditionally provided by banks. This financing activity is considered _shadow_ banking because it is outside the scope of banking regulators. While it can be debated that **disintermediation** (removing banks from the financial intermediation of financing activity) makes the overall market more efficient, it can hardly be debated that such financing activity gets the same regulatory scrutiny as banking in most jurisdictions. So it follows, according to Bisias et al., that "by moving assets off the balance sheets of highly regulated, traditional depositories, and into less regulated" entities, there is the potential for risks, including systemic risk, to emerge unseen by regulators or other market participants.

**Figure 2: Growth of Disintermediation Around the World**

![](https://assets.gathercontent.com/NTQ2NTg/4jfCQAGMJM55nNUg?auto=format%2Ccompress&fit=max&h=800&q=75&s=08162684cb0bd2313cc660e8e69b0489)

Source: Buchholz, Katharina. “Where Shadow Banking is Growing.” _Statista_, 18 Nov 2020, [https://www.statista.com/chart/23534/nbfi-growth-by-country/](https://www.statista.com/chart/23534/nbfi-growth-by-country/).

As these less-regulated and unregulated entities provide more and more financing (as shown in figure 2), they become increasingly important to the financial system as well as a growing source of potential contagions, all the more so since they operate outside the purview of regulators.

## **5. Conclusion**

In this module, we started our journey to comprehend and measure the many risks that lurk in the financial ecosystem. We went beyond the standard market risk measures that focus on equity prices and Value at Risk. In the next module, we continue this journey with the second half of the same reading, where we gain exposure to network analysis, stress testing and scenario analysis, to name just a few of the exciting approaches to come.

**References**

-   Bisias, Dimitrios, et al. “A Survey of Systemic Risk Analytics.” _Annual Review of Financial Economics_, vol. 4, no. 1, Oct 2012, pp. 255–296, [https://dspace.mit.edu/bitstream/handle/1721.1/87772/Lo_A%20survey.pdf?sequence=1&isAllowed=y](https://dspace.mit.edu/bitstream/handle/1721.1/87772/Lo_A%20survey.pdf?sequence=1&isAllowed=y).
-   Buchholz, Katharina. “Where Shadow Banking is Growing.” _Statista_, 18 Nov 2020, [https://www.statista.com/chart/23534/nbfi-growth-by-country/](https://www.statista.com/chart/23534/nbfi-growth-by-country/).
-   Schwarz, Steven. "Regulating Shadow Banking." _Inaugural Address for the Inaugural Symposium of the Review of Banking and Financial Law,_ 2011–2012, [https://scholarship.law.duke.edu/cgi/viewcontent.cgi?article=3121&context=faculty_scholarship](https://scholarship.law.duke.edu/cgi/viewcontent.cgi?article=3121&context=faculty_scholarship).
-   Qi, Ming et al. "Interconnectedness and Systemic Risk Measures of Chinese Financial Institutions." _Kybernetes_, 2022, vol. 51, no. 13, pp. 57–81, [https://www.emerald.com/insight/content/doi/10.1108/K-04-2021-0270/full/html](https://www.emerald.com/insight/content/doi/10.1108/K-04-2021-0270/full/html).



# 1.2.**SYSTEMIC RISK: NETWORKS AND PRINCIPAL COMPONENTS**

**Reading Time**

30 minutes

**Prior Knowledge**

Eigenvalues / eigenvectors

**Keywords**

Historical cost accounting, Mark-to-market accounting, Too big to fail, Fire sale contagion, Resolution plan, Granger causality, Eigenvector centrality measure

_In the first lesson of this course, we saw that financial engineering, with its wide-ranging and innovative techniques, is in many ways uniquely equipped for tackling the complex problem of systemic risk. In this second lesson, we see more examples of these techniques in action._

## **1. Three Additional Categories of Systemic Risk Techniques**

In this second lesson, we walk through three more categories of techniques that were previewed in the last lesson’s required reading, which was just the first half of the same article that is the required reading for this lesson. Specifically, we take a closer look at the following three categories of models from Table 1 of the last lesson's reading (Bisias 3):

-   Granular foundations and network measures
-   Forward-looking risk measurement
-   Stress Tests

To quote Bisias et al. on this point, this lesson will continue to “exploit a finer-grained instrumentation of the financial system and a more detailed understanding of the modalities of institutional stress and failure” (57). For these techniques to be useful for their originally intended risk management purposes and beyond, you should understand the pseudo-code for relevant algorithms. As should be clear by now, the algorithms employed in this domain may help you solve a problem in an area practically distinct from risk management.

## **2. Fire Sale**

The paper by Bisias et al. also reformulates some of the important ideas we have seen in previous courses about fire sale contagion. For example, the authors frame the discussion as a debate about how accounting methods can give rise to vicious circles of liquidation and price deflation (Bisias et al. 73). With “historical cost” accounting, the price of a given security (e.g., stock or bond) as stated on a firm’s balance sheet as an asset is the price actually paid for that security. If the price drops at any time after that initial purchase and before selling it, the balance sheet will not reflect the change. On the other hand, with "mark-to-market accounting," the balance sheet will reflect the most current price of the security: A price drop is reflected as a decrease in assets just as a security price rise is reflected as an increase in assets.

What exactly does this have to do with systemic risk? First, recall the Fundamental Accounting Equation: Assets = Liabilities + Equity. If the value of assets falls, then—assuming no change to the value of liabilities—the value of equity is reduced by that amount. If the value of assets decreases enough, then the value of the equity can be wiped out completely. In that case, as the owners of the firm, the shareholders have an investment worth zero. Consider that the firm could be profitable—“business is good,” revenue is up, profits are stable or even high—but none of that matters and the firm is bankrupt because of the market dynamics affecting the price of stocks and/or bonds in their holdings. (An example might be a bank that has a strong income from its customer base but nevertheless has market exposure via its investment holdings.) Obviously, with historical cost accounting, this would not be the case because the balance sheet view of the shareholders' equity is immune to the security price changes.

That is just the first-order effect of mark-to-market accounting. Consider these scenarios:

-   Let’s say Firm 1 notices that the prices of certain stocks ABC and DEF are declining, and so it defensively starts selling stocks ABC and DEF before they drop in price even more. This selling puts additional downward pressure on the prices of ABC and DEF. Now let's add Firm 2, which _was_ comfortable holding positions in stocks ABC and DEF despite the initial price decreases, but the second wave of price decreases caused by Firm 1 and firms similar to Firm 1 brought the price down so much that Firm 2 and similar firms must now also sell defensively. This is the pro-cyclical effect of mark-to-market accounting.
-   Let’s say instead that Firm 1 notices that the prices of certain stocks ABC and DEF are declining, but it does not want to sell these stocks at these low prices because the prices might rebound. Firm 1 does not want to “lock in” losses on these stocks, so instead it starts selling stocks XYZ and UVW. The downward pressure caused by Firm 1 and similar firms causes the stock prices of XYZ and UVW to fall. Now XYZ and UVW prices are falling for the simple reason that selling them is more profitable. That is, their prices are falling only because they had not already fallen.
-   Let's introduce Firm 3 and say that the balance sheet of Firm 3 was always highly levered, meaning a high asset value but a low equity value. Firm 3 sees its equity value decreasing very quickly due to the leverage effect of the asset values, so—as if the house were on fire—everything must be sold as quickly as possible. This is a fire sale, and as we see in the first two scenarios, it is contagious. This is the meaning of “fire-sale contagion” (Bisias et al. 73).

Recall that we have seen this disastrous pro-cyclical effect before: We discussed it with respect to real estate and the Great Financial Crisis in the Financial Markets course. We discussed it with respect to the crowded trades that can result from factor investing, as in the Quant Meltdown of August 2007, in the Portfolio Management course.

## **3. Contingent Claims and "Too Big to Fail"**

Now imagine that the balance sheet of Firm 3 is orders of magnitude larger than most other firms. The government recognizes that if Firm 3 must liquidate its balance sheet in a fire sale due to bankruptcy, the contagion effect described above would force many other firms to go bankrupt as well. If the government decides that the impact of this cascade effect would be too horrendous for its citizens and/or its economy (or those of the world at large), it might take some action to prevent Firm 3 from going bankrupt. An example would be an equity injection or government guarantee for Firm 3’s liabilities in order to avoid or postpone bankruptcy. In such a case, Firm 3 would be considered “too big to fail.” To the extent that a government recognizes a firm as “too big to fail," that firms enjoys an _implicit_ government guarantee. The value of this _implicit_ guarantee is _explicitly_ calculated by Bisias et al. in the section “Contingent Claims Analysis” (77–80). The “contingent claims” refers to the claims (in the form of liabilities) on the government that are _contingent_ on such a rescue. Remember that there may be no _explicit_ guarantee: The government may not have contractually agreed to act as guarantor for Firm 3 in advance (the government may even announce that it will _not_ intervene to help distressed firms), but the market is nevertheless betting that the government would not permit through its own inaction the widespread damage that Firm 3’s bankruptcy would cause.

If Firm 3 believes that the government will intervene to prevent its bankruptcy, Firm 3 might conduct itself less prudently or even recklessly. Why not take on more risk if you know the consequences will be softened? Recall again from the Financial Markets course how moral hazard works: If an entity does not bear the costs of its own behavior, it is more likely to behave in a riskier fashion.

Exactly to circumvent this situation where the government feels obliged to step in and save a large failing firm, such as a large bank holding company, regulations like the Dodd-Frank Act in the United States require many firms to construct and report their “resolution plan”: Also called “living wills,” such plans “describe the company’s strategy for rapid and orderly resolution in the event of material financial distress or failure of the company” (Federal Reserve).

See the [documentation](https://frds.io/measures/cca/) for the Python library frds for a trivial implementation using the function `cca`, which takes the following as parameters, like you would expect after doing the required reading:

-   Equity (float) — the market value of the equity of the firm.
-   Volatility (float) — the volatility of equity.
-   Risk_free_rate (float) — the risk-free rate in annualized terms.
-   Default_barrier (float) — the face value of the outstanding debt at maturity.
-   Time_to_maturity (float) — the time to maturity of the debt.
-   Cds_spread (float) — the CDS spread for the firm."

(Excerpted from Financial Research Data Services, [https://frds.io/measures/cca/](https://frds.io/measures/cca/))

## **4. Network Analysis**

In the example scenarios above, we simplistically generalized about Firms 1, 2, and 3; we did not even start to account for the complexity that we touched on in the previous lesson notes. In the reading for this lesson, however, we finally have the models and accompanying formulas to evaluate how risk builds up and spreads through a financial system. Perhaps the most interesting approach to this is network analysis, which we touched on in the last lesson as well as in the Portfolio Theory course. As in the previous discussions of network analysis and graph theory, the connections between two nodes in a network are determined by the amount of information that flows between them, where information can be quantified in many different ways. In this lesson, however, we are less interested in general correlation because we want to understand the causes of risks: We want to track down the sources of risks, and how they flow directionally. Correlation is just a measure of association (correlation does not imply causality), so we require a measure that specifically captures causality or at least “predictive causality.” Granger causality is one popular test for whether a predictive causality relationship exists. “According to Granger causality, if a signal X1 "Granger-causes" (or "G-causes") a signal X2, then past values of X1 should contain information that helps predict X2 above and beyond the information contained in past values of X2 alone” (Scholarpedia). To compute Granger causality is straightforward with the statsmodels library:

```
import models.api as sm

from models.tsa.stattools import grangercausalitytests

import numpy as np

data = sm.datasets.macrodata.load_pandas()

data = data.data[["realgdp", "realcons"]].pct_change().dropna()

gc_res = grangercausalitytests(data, 3)
```

(Adapted from Perktold "Granger")

In the dataset referenced above, `realgdp` is the real annual gross domestic product and `realcons` is the real annual personal consumption expenditures (Perktold "Macrodata"). The code below uses the full macroeconomic dataset (not just `realgdp` and `realcons`) and outputs a full Granger causation matrix for all the data columns.

```
df = pd.DataFrame.from_records(sm.datasets.macrodata.load().data)

def granger_causation_matrix(data, variables,test,verbose=False):

 x = pd.DataFrame(np.zeros((len(variables),len(variables))), columns=variables,index=variables)

 for c in x.columns:

 for r in x.index:

 test_result = grangercausalitytests(data[[r,c]], maxlag=maxlag, verbose=False)

 p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]

 if verbose:

 print(f'Y = {r}, X= {c},P Values = {p_values}')

  min_p_value = np.min(p_values)

 x.loc[r,c] = min_p_value

 x.columns = [var + '_x' for var in variables]

 x.index = [var + '_y' for var in variables]

 return x

maxlag = 4

granger_causation_matrix(df, df.columns, 'ssr_ftest')
```

(Adapted from Polpanumas)

Remember when interpreting this output matrix that

-   The test is whether the variable with the “_x” suffix G-causes the variable with the “_y” suffix.
-   Smaller p-values suggest a higher likelihood of Granger causality.

## **5. Principal Components Analysis**

With this Granger causality matrix in hand, we can perform principal components analysis (PCA) and calculate three systemic risk measures described by Bisias et al.:

1) Bisias et al. describe summing the exposure of a firm to the top 20 principal components (68).

This particular metric may not in fact be that useful in most situations. Ask yourself under what circumstances you would have at your disposal the kind of exposure data it requires. Possibly it would make sense for a regulator, but even then, large institutions need a significant amount of time to collect and prepare such detailed exposure information. As a result, by the time such information is collected and analyzed, the metric may indicate that systemic risk levels are not worrying, but circumstances have actually worsened significantly since the time and day of the exposure snapshot. Alternatively, the metric may indicate that systemic risk levels _are_ worrying but this warning comes too late.

Also bear in mind that there is nothing magical about the number 20, so this number should be considered a hyperparameter that can be tuned based on the size of the graph or matrix being analyzed, i.e., the number of entities under examination.

2) Absorption ratio (AR): A similar analysis but from a different perspective, the AR indicates the proportion of the total variance that can be explained (or “absorbed”) by some predefined number of eigenvectors (Bisias et al. 98)—and again, this number should be considered a hyperparameter, to be tuned with research and experience. This metric should remind you of de Prado’s de-toning technique discussed in Portfolio Theory: Back then, we assumed that the largest eigenvalue-eigenvector pair was associated with the market, the equivalent of the market beta factor from the Capital Asset Pricing Model (de Prado 31). We often refer to the market beta factor as the undiversifiable systemic risk. With the AR, we are on the lookout for when the eigenvectors with the highest eigenvalues explain an increased amount of the variation in returns. This would imply that some new “factor” or set of factors, analogous to the market factor in the Capital Asset Pricing Model, is driving returns. Because these factors could drive returns downwards in a different, possibly more extreme way than just the market beta, they represent systemic risk. The AR helps identify the presence of such “factors” by defining them as eigenvectors with increasing eigenvalues.

See also the documentation for the Python library frds for a trivial implementation using `absorption_ratio(data)` where data is simply the matrix of asset returns.

Note: Pay attention to the context of the abbreviation “AR.” When it is not used in connection with this specific systemic risk measure, it means auto-regression, as is the case in the rest of the paper by Bisias et al. when they refer more generally to “an AR model.”

3) Eigenvector Centrality: This measure actually “computes the centrality for a node based on the centrality of its neighbors” and we can use the function `eigenvector_centrality` from the Python library NetworkX (Hagberg EC). The definition of this measure as described in the NetworkX documentation is more detailed and accurate than the one in Bisias et al. Nevertheless, it does require that we transform the Granger causality matrix into an adjacency matrix, which is a matrix of ones where there is Granger causality and zeros elsewhere. We then convert this matrix into a graph with `from_numpy_matrix` from the same library.

## **6. Conclusion**

In this lesson, we looked closely at some of the quantitative techniques used to estimate or predict systemic risk events, such as principal components analysis. We also discussed the network effects of "too big to fail" firms and fire sale contagion. In the next lesson, we discuss "cross-sectional" measures of systemic risk (CoVaR, Distressed Insurance Premium, Co-Risk, Marginal and Systemic Expected Shortfall) as well as measures of the market's illiquidity.

**References**

-   Bisias, Dimitrios et al. “A Survey of Systemic Risk Analytics.” _Annual Review of Financial Economics_, vol. 4, no. 1, Oct 2012, pp. 255–296, [https://dspace.mit.edu/bitstream/handle/1721.1/87772/Lo_A%20survey.pdf?sequence=1&isAllowed=y](https://dspace.mit.edu/bitstream/handle/1721.1/87772/Lo_A%20survey.pdf?sequence=1&isAllowed=y).
    
-   Board of Governors of the Federal Reserve System. "Living Wills (or Resolution Plans)." _The Federal Reserve_, [https://www.federalreserve.gov/supervisionreg/resolution-plans.htm#](https://www.federalreserve.gov/supervisionreg/resolution-plans.htm#).
    
-   Financial Research Data Services. "Contingent Claim Analysis." [https://frds.io/measures/cca/](https://frds.io/measures/cca/)
    
-   Hagberg, Aric A et al. “Exploring Network Structure, Dynamics, and Function using NetworkX.” In: _Proceedings of the 7th Python in Science Conference (SciPy2008)_ by Gäel Varoquaux, Travis Vaught, and Jarrod Millman (Eds), (Pasadena, CA, USA), pp. 11–15, 2008, [https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.eigenvector_centrality.html](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.eigenvector_centrality.html)
    
-   Seth, Anil. "Granger Causality." _Scholarpedia_, [http://www.scholarpedia.org/article/Granger_causality#:~:text=Granger%20causality%20is%20a%20statistical,values%20of%20X2%20alone](http://www.scholarpedia.org/article/Granger_causality#:~:text=Granger%20causality%20is%20a%20statistical,values%20of%20X2%20alone).
    
-   Perktold, Josef et al. “United States Macroeconomic data.” _models.org_, [https://www.statsmodels.org/0.6.1/datasets/generated/macrodata.html](https://www.statsmodels.org/0.6.1/datasets/generated/macrodata.html).
    
-   Perktold, Josef et al. “models.tsa.stattools.grangercausalitytests.” _models.org_, [https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.grangercausalitytests.html](https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.grangercausalitytests.html)
    
-   Polpanumas, Charin. "granger." _GitHub_, [https://github.com/cstorm125/granger/blob/master/trend_granger.ipynb](https://github.com/cstorm125/granger/blob/master/trend_granger.ipynb).