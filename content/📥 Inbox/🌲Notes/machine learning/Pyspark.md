- [[Spark]] is a `system` for working with huge quantities of data across multiple compute servers at the same time. 
	- it allows you to run computations parallely instead of sequentially
- Pyspark is a python API for using Spark 
- wrt [[big data]], we call data big, when one server cant hold all the data, let alone have the RAM to handle that big data, so it needs to be distributed. which is where [[hadoop]] based tools/applications come in
- concepts:
	- [[distributed computing]] 


### Resources: 
- Youtube: 
	- The ONLY PySpark Tutorial You Will Ever Need. [link](https://www.youtube.com/watch?v=cZS5xYYIPzk&ab_channel=MoranReznik) #halfdone 



