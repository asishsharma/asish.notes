# Annotations  
(7/20/2022, 10:11:15 PM)

“multilayer perceptron, Bayesian neural networks, radial basis functions, generalized regression neural networks (also called kernel regression), K-nearest neighbor regression, CART regression trees, support vector regression, and Gaussian processes” ([Ahmed et al., 2010, p. 596](zotero://select/library/items/4U7S7APE)) ([pdf](zotero://open-pdf/library/items/MGL8TJNF?page=5&annotation=VLUKGFI6)) multilayer perceptron is just another name for neural network.  
In the WQU course, we extensively use k-regression, cart, svm. gaussian process only a mention.

“sion. BNN’s have enjoyed wide applicability in many areas such as economics/finance (Gencay and Qi, 2001) and engineering (Bishop, 1995). The idea of BNN is to” ([Ahmed et al., 2010, p. 598](zotero://select/library/items/4U7S7APE)) ([pdf](zotero://open-pdf/library/items/MGL8TJNF?page=7&annotation=RG3M2XFF))

“and parameter, respectively. The term p(D | ) in (7) is obtained by a quadratic approximation of J in terms of the weights and then integrating out the weights. We used the Matlab version version ‘trainbr’ for BNN (applied to a multilayer perceptron architecture). This routine is based on the algorithm proposed by Foresee and Hagan (1997). This algorithm utilizes the Hessian that is obtained any way in the LevenbergMarquardt optimization algorithm in approximating (7).” ([Ahmed et al., 2010, p. 599](zotero://select/library/items/4U7S7APE)) ([pdf](zotero://open-pdf/library/items/MGL8TJNF?page=8&annotation=M7T4ZCLW))

“ion estimator. In the machine learning community, the term generalized regression neural network (or GRNN) is typically used. We will use this latter term. The GRNN model is a nonparametric model where the prediction for a given data point x is given by the average of the target outputs of the training data points in the vicinity of the given point x (Hardle, 1990). The local average is constructed by weighting the points according to their distance from x, using some kernel function. The estimation is just the weighted sum of the observed res” ([Ahmed et al., 2010, p. 600](zotero://select/library/items/4U7S7APE)) ([pdf](zotero://open-pdf/library/items/MGL8TJNF?page=9&annotation=ULT8QTG8))

“in the training set. We then pick the closest K training data points and set the prediction as the average of the targe” ([Ahmed et al., 2010, p. 601](zotero://select/library/items/4U7S7APE)) ([pdf](zotero://open-pdf/library/items/MGL8TJNF?page=10&annotation=JPQ5F4UD))

\[image\] ([pdf](zotero://open-pdf/library/items/MGL8TJNF?page=10&annotation=3PUUJ6KA))  
([Ahmed et al., 2010, p. 601](zotero://select/library/items/4U7S7APE))